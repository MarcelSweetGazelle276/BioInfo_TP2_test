# -*- coding: utf-8 -*-
"""agc-tp

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13QDQOt3ZAEtdkoiAEwEpxs0gViCZcV8H
"""

#!/bin/env python3
# -*- coding: utf-8 -*-
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#    A copy of the GNU General Public License is available at
#    http://www.gnu.org/licenses/gpl-3.0.html




"""OTU clustering"""

import argparse
import sys
import os
import gzip
import statistics
from collections import Counter
# https://github.com/briney/nwalign3
# ftp://ftp.ncbi.nih.gov/blast/matrices/
import nwalign3 as nw

__author__ = "Your Name"
__copyright__ = "Universite Paris Diderot"
__credits__ = ["Your Name"]
__license__ = "GPL"
__version__ = "1.0.0"
__maintainer__ = "Your Name"
__email__ = "your@email.fr"
__status__ = "Developpement"


def isfile(path):
    """Check if path is an existing file.
      :Parameters:
          path: Path to the file
    """
    if not os.path.isfile(path):
        if os.path.isdir(path):
            msg = "{0} is a directory".format(path)
        else:
            msg = "{0} does not exist.".format(path)
        raise argparse.ArgumentTypeError(msg)
    return path


def get_arguments():
    """Retrieves the arguments of the program.
      Returns: An object that contains the arguments
    """
    # Parsing arguments
    parser = argparse.ArgumentParser(description=__doc__, usage=
                                     "{0} -h"
                                     .format(sys.argv[0]))
    parser.add_argument('-i', '-amplicon_file', dest='amplicon_file', type=isfile, required=True, 
                        help="Amplicon is a compressed fasta file (.fasta.gz)")
    parser.add_argument('-s', '-minseqlen', dest='minseqlen', type=int, default = 400,
                        help="Minimum sequence length for dereplication")
    parser.add_argument('-m', '-mincount', dest='mincount', type=int, default = 10,
                        help="Minimum count for dereplication")
    parser.add_argument('-c', '-chunk_size', dest='chunk_size', type=int, default = 100,
                        help="Chunk size for dereplication")
    parser.add_argument('-k', '-kmer_size', dest='kmer_size', type=int, default = 8,
                        help="kmer size for dereplication")
    parser.add_argument('-o', '-output_file', dest='output_file', type=str,
                        default="OTU.fasta", help="Output file")
    return parser.parse_args()

def read_fasta(amplicon_file, minseqlen):
    with gzip.open(amplicon_file, "rt") as fichier:
        seq = ""
        seq_list = []
        for line in fichier:
            if not line.lstrip().startswith('>'):
                seq += line.strip()
            else:
                if len(seq) != 0:
                    seq_list.append(seq)
                seq = ""
    if len(seq) != 0:
        seq_list.append(seq)
    for seq in seq_list:
        if len(seq) >= minseqlen:
            yield seq


def read_fasta_new(amplicon_file,minseqlen):
  with gzip.open(amplicon_file, "rt") as seqfile:
    name, seq = None, []
    for line in seqfile:
      line = line.rstrip()
      if line.startswith(">"):
        if name:
          if len(seq[0])>=minseqlen:
            yield(seq[0])
        name, seq = line, []
      else:
        seq.append(line)
    if name:
      if len(seq[0])>=minseqlen:
        yield(seq[0])

def dereplication_fulllength_1(amplicon_file, minseqlen, mincount):
	seq_list=[]
	for element in read_fasta(amplicon_file,minseqlen):
		seq_list.append(element)
	occ=[[x,seq_list.count(x)] for x in set(seq_list)]
	occ.sort(key=lambda x:x[1],reverse=True)
	for thing in occ:
		if thing[1]>=mincount:
			yield thing

def dereplication_fulllength_2(amplicon_file, minseqlen, mincount):
	seq_uniq=[]
	occ=[]

	seq_gen=read_fasta(amplicon_file, minseqlen)
	for seq in seq_gen:
		if seq not in seq_uniq:
			seq_uniq.append(seq)
			occ.append(1)
		else:
			index=seq_uniq.index(seq)
			occ[index]+=1
	zipped=sorted(zip(occ,seq_uniq),reverse=True)
	uniq_sorted=[seq for _,seq in zipped]
	occ_sorted=[oc for oc, _ in zipped]
	print('exec')
	for i in range(len(occ_sorted)):
		if occ_sorted[i]>mincount:
			yield [uniq_sorted[i], occ_sorted[i]]

def dereplication_fulllength(amplicon_file, minseqlen, mincount):
	list_gen=read_fasta(amplicon_file, minseqlen)
	dictr={}
	for seq in list_gen:
		if seq not in dictr:
			dictr[seq] = 1
		else:
			dictr[seq] += 1
	for sequence, count in sorted(dictr.items(), key=lambda item: item[1], reverse=True):
		if count >= mincount:
			yield [sequence, count]
				
def get_chunks(sequence, chunk_size):
	new=[]
	for i in range(0,len(sequence),chunk_size):
		if len(sequence[i:i+chunk_size])==chunk_size:
			new.append(sequence[i:i+chunk_size])
	return new



def get_unique(ids):
    return {}.fromkeys(ids).keys()

#@def cut_kmer(sequence, kmer_size):
#'''returns a gen of allkmers of size kmer size in seq'''
#	unique_kmers=[]
#	for i in range(len(sequence)):
#		kmer=sequence(

def cut_kmer(sequence, kmer_size):
	for i in range(0,len(sequence)-kmer_size+1):
		yield sequence[i:i+kmer_size]

def cut_kmer_1(sequence, kmer_size):
	unique=[]
	for i in range(len(sequence)):
		kmer=sequence[i:i+kmer_size]
		if kmer not in unique and len(kmer)==kmer_size:
			yield kmer



#GET UNIQUE KMER/ get_unique_kmer (same fuction)
def get_kmer_dict(kmer_dict,sequence,id_seq,kmer_size):
	cut_kmer_r=cut_kmer(sequence, kmer_size)
	for kmer in cut_kmer_r:
		if kmer not in list(kmer_dict.keys()):
			kmer_dict[kmer]=[id_seq]
		else:
			if id_seq not in kmer_dict[kmer]:
				#kmer_dict[kmer]=kmer_dict[kmer].append(id_seq)
				kmer_dict[kmer].append(id_seq)
	return kmer_dict

def common(lst1, lst2): 
    return list(set(lst1) & set(lst2))

def get_identity(alignment_list):
	compteur = 0
	for element in range(0,len(alignment_list[0]),1):
		if alignment_list[0][element] == alignment_list[1][element]:
			compteur=compteur+1
	return round(compteur/len(alignment_list[0]),3)*100

def detect_chimera(perc_identity_matrix):
    """
    """
    som_et = 0
    seq1 = False
    seq2 = False

    for i in range(len(perc_identity_matrix)):
        som_et += statistics.stdev(perc_identity_matrix[i])
        if perc_identity_matrix[i][0] > perc_identity_matrix[i][1]:
            seq1 = True
        if perc_identity_matrix[i][0] < perc_identity_matrix[i][1]:
            seq2 = True
    if len(perc_identity_matrix) !=0:
    	if som_et/(len(perc_identity_matrix)) > 5 and seq1 and seq2:
        	return True
    	else :
        	return False


def search_mates(kmer_dict, sequence, kmer_size):
    """
	return a list of 8 first sequences which ressembles the netry sequence the most. If there's less than 8 sequences, it will return only those found
    """
    List=[ids for kmer in cut_kmer(sequence, kmer_size) if kmer in kmer_dict for ids in kmer_dict.values()]
    newlist=map(tuple,List)
    count=Counter(newlist)
    mc=count.most_common(8)
    print(count.most_common(8))
    l=[]
    for elt in mc:
    	l.append(list(elt[0]))
    print(l)
    res=list({x for elt in l for x in elt})
    print(res)
    return res
    
def chimera_removal(amplicon_file, minseqlen, mincount, chunk_size, kmer_size):
    """
    """
    dfr_lst = dereplication_fulllength(amplicon_file, minseqlen, mincount)
    dfr_lst2 = dereplication_fulllength(amplicon_file, minseqlen, mincount)
    kmer_dict = {}
    com = []
    id=1
    for seq in dfr_lst:
    	kmer_dict = get_kmer_dict(kmer_dict,seq[0],id,kmer_size)
    	id+=1
    	print('/n STARTING NEW SEQUENCE /n')
    	print(seq)
    perc_identity_matrix = []

    for l in dfr_lst2:
        chunks = get_chunks(l[0], chunk_size)
        print("CHUNKSSSSSSSSSSSSSSSSSSSSSSS")
        print(chunks)
        chunk_mates = []
        for seq in chunks:
            mates = search_mates(kmer_dict, seq, kmer_size)
            chunk_mates.append(mates)
        print('/n CHUNK MATES: /n')
        print(chunk_mates)

        if len(chunk_mates) > 1:
            for f in com[0:2]:
                sequ = get_chunks(no_chimere[f], chunk_size)
                perc_identity_matrix = [[]]
                for k, chunk in enumerate(chunks):
                    align = nw.global_align(chunk, sequ[k])
                    identite =  get_identity(align)
                    perc_identity_matrix[k].append(identite)
            chimera = detect_chimera(perc_identity_matrix)
            print("KURWA MAC")
            print(perc_identity_matrix)

        if not detect_chimera(perc_identity_matrix):
            yield l
			


def abundance_greedy_clustering(amplicon_file, minseqlen, mincount, chunk_size, kmer_size):
    """
    """
    OTU = []

    lst = [chimera_removal(amplicon_file, minseqlen, mincount, chunk_size, kmer_size)]

    for seq, count in lst:
        OTU.append([seq, count])

    return OTU

def fill(text, width=80):
    """Split text with a line return to respect fasta format"""
    return os.linesep.join(text[i:i+width] for i in range(0, len(text), width))

def write_OTU(OTU_list, output_file):
    """
    """
    with open(output_file, "w") as f:
        for i, o in enumerate(OTU_list):
            f.write(">OTU_{"+str(i+1)+"} occurence:{"+str(o[1])+"}\n"+o[0])


#==============================================================
# Main program
#==============================================================
def main():
    """
    Main program function
    """
    # Get arguments
    args = get_arguments()


if __name__ == '__main__':
    main()



